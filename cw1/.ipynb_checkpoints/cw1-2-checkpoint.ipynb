{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "## 6.1  Define the Task\n",
    "\n",
    "#### Dataset Information\n",
    "Dataset will be the cifar100 dataset from tensorflow. This is a multi-class classification problem with 100 different classes of images to be classified. There are 500 training images and 100 testing images for each of the 100 classes so this is a balanced dataset.\n",
    "\n",
    "#### Accuracy Metric\n",
    "Balanced classification problems such as this a good measure of success will be accuracy and area under the reciever operating characteristic curve. Guessing all one type of class would lead to an accuracy of 1% since there are 100 different classes. \n",
    "\n",
    "#### Evaluation\n",
    "The model will be evaluated using a holdout test set of 10,000 images consisting of 100 of each class. The training set will consist of 500 images of each class for a total of 50,000 images.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Show data class information metrics\n",
    "## show that the counts of the labels are balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is just like the CIFAR-10, except it has 100 classes containing 600 images each. There are 500 training images and 100 testing images per class. The 100 classes in the CIFAR-100 are grouped into 20 superclasses. Each image comes with a \"fine\" label (the class to which it belongs) and a \"coarse\" label (the superclass to which it belongs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Develop a Model\n",
    "### Data Preparation\n",
    "\n",
    "The data consists of images that are 32x32.  There are 50,000 training images and 10,000 test images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
      "169001437/169001437 [==============================] - 52s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = cifar100.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor shape\n",
      "\ttraining images: (50000, 32, 32, 3)\n",
      "\ttraining labels: (50000, 1)\n",
      "\ttraining images: uint8\n",
      "\ttraining labels: int32\n",
      "\ttest images: (10000, 32, 32, 3)\n",
      "\ttest labels: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "print('tensor shape')\n",
    "print('\\ttraining images:', train_images.shape)\n",
    "print('\\ttraining labels:', train_labels.shape)\n",
    "print('\\ttraining images:', train_images.dtype)\n",
    "print('\\ttraining labels:', train_labels.dtype)\n",
    "print('\\ttest images:', test_images.shape)\n",
    "print('\\ttest labels:', test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess\n",
    "1. Reshape to flatten 32x32x3 to vector\n",
    "2. Cast vector as floats\n",
    "3. Rescale from [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_flat = train_images.reshape((50000, 32*32*3)).astype('float32') / 255.\n",
    "test_images_flat = test_images.reshape((10000, 32*32*3)).astype('float32') / 255.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images_flat[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[19],\n",
       "       [29],\n",
       "       [ 0],\n",
       "       ...,\n",
       "       [ 3],\n",
       "       [ 7],\n",
       "       [73]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "orig_label = train_labels[0]\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'[19]'as one-hot vector:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('\\'', orig_label, '\\'', 'as one-hot vector:\\n', train_labels[0], sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model\n",
    "Start smal, smallest amout of layers can do and small # of units then increase size\n",
    "\n",
    "Only a baseline model if it has \"statistical power\" meaning ??\n",
    "\n",
    "**Selecting loss function**\n",
    "Chose categorical crossentropy but why?\n",
    "General loss function for classification tasks where the evaluation metric is RCO AUC - need to figure out why that is. Used as a proxy, hope is that the lower the crossentropy the higher the ROC AUC will be \n",
    "\n",
    "**Final Layer**  \n",
    "Multi-class single label classification => \n",
    "* last layer activation: **softmax**  \n",
    "* Loss function: **categorical_crossentropy**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build\n",
    "from tensorflow.keras import models, layers\n",
    "\n",
    "# create empty network\n",
    "network = models.Sequential()\n",
    "\n",
    "# add 2 layers\n",
    "network.add(layers.Dense(128, activation='relu', input_shape=(32 * 32 * 3, )))\n",
    "network.add(layers.Dense(100, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.compile(optimizer='rmsprop',\n",
    "               loss='categorical_crossentropy', \n",
    "               metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "391/391 [==============================] - 5s 10ms/step - loss: 4.3955 - accuracy: 0.0431\n",
      "Epoch 2/5\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 4.0210 - accuracy: 0.0844\n",
      "Epoch 3/5\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 3.8878 - accuracy: 0.1051\n",
      "Epoch 4/5\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 3.7975 - accuracy: 0.1245\n",
      "Epoch 5/5\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 3.7413 - accuracy: 0.1348\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x216857907c0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.fit(train_images_flat, train_labels, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.4 Develop a model that overfits\n",
    "Once have model with statistical power => question is now is model sufficiently powerful?  \n",
    "Enough layers? Enough Parameters? to properly model problem at hand  \n",
    "Universal tension in ML is optimization vs generalization  \n",
    "\n",
    "To figure out how big a model must develop model that overfits  \n",
    "1. Add layers\n",
    "2. make layers bigger\n",
    "3. train for more epochs\n",
    "\n",
    "Monitor the training loss and validation loss as well as training and validation values for any metrics that you care about  \n",
    "\n",
    "**Overfitting:** Once see that model's performance on validation data begins to degrade\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.5 Hyperparamater tuning\n",
    "\n",
    "goal now is to maximize generalization performance  \n",
    "repeatedly modify model and train it and evaluate it (**on evaluation data NOT test data**)  \n",
    "\n",
    "\n",
    "\n",
    "**Search Space**\n",
    "1. learning rate\n",
    "2. units\n",
    "3. layers\n",
    "    ii. add or remove layers\n",
    "4. add dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "2fc6efb3340bf4aae142c4471c3414bb5b17e6e80ba42a259676c40f0503db89"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
